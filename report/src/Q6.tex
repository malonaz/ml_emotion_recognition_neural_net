For this question, we have decided to use the Tensorflow learning library and Kera's API.
We decided on the architecture [convolution - relu] x 2 - [affine - relu - dropout] - softmax.
In order to find the optimal parameters, we ran a random search on imperial's Condor system,
letting hidden units run between 128 and 1024, convolutional kernels dimensions run between 2 and 10, dropout rate between 0 and 1,
filter number between 16 and 128, stride number between 1 and 2 and randomly choosing between SGD, Adam or SGD with momentum.
The optimal hyperparameters that we find were 60 filters, 2 units strides, Adam optimizer, and 1003 hidden units.
\begin{itemize}

\item \textbf{Performance of the network with the optimal set of parameters}:
  Having trained a network on the optimal set of parameters,
  we begin by presenting our model's classification error rates on each set.
  \include*{../nets/deep_net/info}
  We now set out to test this model on the FER2013's testing set.
  Please find below a confusion matrix, precision and recall rates, as well as F1 scores per class.
  \begin{figure}[h]
  \begin{center}
    \caption{Confusion Matrix for test dataset}
    \begin{tabular}{ | l || c | c | c | c | c | c | c |}
    \hline
          & Angry 1 & Disgust 2 & Fear 3 & Happy 4 & Sad 5 & Surprise 6 & Neutral 7 \\ \hline \hline
        Angry 1 & 139 & 5 & 59 & 80 & 89 & 18 & 77 \\ \hline
        Disgust 2 & 18 & 16 & 7 & 6 & 6 & 0 & 3 \\ \hline
        Fear 3 & 45 & 4 & 124 & 71 & 122 & 56 & 74 \\ \hline
        Happy 4 & 35 & 0 & 45 & 645 & 72 & 14 & 84 \\ \hline
        Sad 5 & 74 & 3 & 68 & 112 & 255 & 15 & 126 \\ \hline
        Surprise 6 & 12 & 3 & 46 & 36 & 43 & 248 & 27 \\ \hline
        Neutral 7 & 61 & 4 & 40 & 122 & 129 & 19 & 232 \\ \hline
    \end{tabular}
    \label{fig:confusionMatrix}
\end{center}
\end{figure}

  
\begin{figure}[h]
\begin{center}
\caption{Recall and Precision Rates for test dataset}
\begin{tabular}{ | l || c | c | c | c | c | c | c |}
  \hline
         & Angry 1 & Disgust 2 & Fear 3 & Happy 4 & Sad 5 & Surprise 6 & Neutral 7 \\ \hline \hline
        Avg Recall(\%) & 29.764 &28.571 &25.000 &72.067 &39.051 &59.759 &38.221 \\ \hline
        Avg Precision(\%) & 36.198 &45.714 &31.877 &60.168 &35.615 &67.027 &37.239 \\ \hline
        F\textsubscript{1} Score(\%) & 32.667 &35.165 &28.023 &65.582 &37.253 &63.185 &37.724 \\ \hline
    \end{tabular}
    \label{fig:averageRecall}
\end{center}
\end{figure}


\item \textbf{Compare the performance of your CNN with the feedforward NN}:\\
  The convolutional neural network outperforms our feedforward neural network significantly on the validation set,
  yet it appears to be prone to overfitting, and thus barely outperforms the feedforward neural net on the testing set.
  Analyzing the confusion matrices gives us a lot more insight into what happens.
  In a strange turn of events, examples of Angry, Disgust and Neutral emotions were misclassified more often with the CNN,
  than they were with the NN.
  Other emotions however, such as Happy, were classified correctly more often with the CNN.
  Looking at the F1 scores, we notice that Fear has taken the biggest hit with this introduction of convolutional layers.
  Since Fear gets confused often with Fear and Sad, it is possible that the convolutional layers have focused on
  a particular area of the face that changes similarly for all three of these emotions.
  With more time on our hands, it would have been interesting to see how far we could push these CNNs.

\item \textbf{How to retrieve our best trained model}:\\
  You may retrieve and test our model using our file \emph{src/test\_deep\_net.py}.
  
\end{itemize}
