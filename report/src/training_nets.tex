But how does a neural net know what weights it should apply to each input?
How are they initialized?
How do they evolve?

   \item Softmax Classifier
   
    
   \item Softmax classifier\\
     Every hidden layer's neurons applies the affine transformation, followed by the ReLU activation function.
     The output layer on the other hand,  applies the affine transformation, followed by the softmax classifer.
     The softmax classifier 




   \item Dropout\\
     Consider a neural network with a large number of hidden units relative to the input layer's size.
     Such a network is a powerful machine learning system, yet, without regularization, it is prone to overfitting.
     Neurons fron a layer develop dependencies to neurons from the previous layers.
     In order to prevent such a thing, we randomly shutdown neurons by setting their outputs to 0.
     This penalizes neurons co-dependencies and yields higher results.
     Let's explore how we determine whether a neuron should be ``dropped'' and how the dropous process affects the 

Once the transformations have been made, we can calculate the error for each output neuron. We can use the squarred error function and sum the erros:

$E = \sum_{k=0}^{N} 1/2*(target - output)^2$.

One of the main idea behind the backpropagation algorithm is to reduce the error rate according to the results expected. We need then to send the output back into the neural network to update parameters such as weight and biais. 

Our algorithm calculates dW and db according to dout, which the first neural network output.
We then need to send the "re-estimated" value in other hidden layer by calculating dX.

