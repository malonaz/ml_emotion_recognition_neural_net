The aim of this coursework is to make us implement a Fully-Connected Neural Network from scratch and train it using backpropagation. 
A Fully-Connected Neural network is made up of one input layer, N hidden layers and one output layers - each hidden layer being made up of elementary layer.
A layer take the output and transforme it in a new output for the follwing path. \\
For this work we first implemented Linear, ReLU and Dropout Layers (forward and backward pass) to be able to create our Network. Then we implemented Softmax Classifier and hyper_parameter optimisation to improve our result and hence our network. \\
In this report we will explain and discuss choices we made on our function implementation and the results we got from the training.

