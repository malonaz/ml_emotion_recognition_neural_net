How does a neural net generate an output or in our case, how does it decide which emotion to recognize?
The output is produced in a process known as the emph{forward pass}.
Let us take a closer look at its inner workings.

\begin{enumerate}
\item Forward pass

We have implemented our neural net to take in batches of data points rather than training it one data point at a time.
As such, the forward pass takes in N examples in a matrix \emph{X}, where each row contains a example of dimensions D.

The forward pass takes a dat
For a given set of data X, weight W and bias b computing the forward pass for the linear function consist in calculating the conresponding  $net = \sum_{k=0}^{N} w_i * x_i + b $.
Then the activation function $\sigma\ $is just as $\sigma\ (net) = net$ .

The idea of the implementation is to apply this transformationt to all data included in the matrix input, and repeat the process for each transformed output. 

Our code has been created to compute every kind of data which includes shape differencies.



\item Backward pass :

Once the transformations have been made, we can calculate the error for each output neuron. We can use the squarred error function and sum the erros:

$E = \sum_{k=0}^{N} 1/2*(target - output)^2$.

One of the main idea behind the backpropagation algorithm is to reduce the error rate according to the results expected. We need then to send the output back into the neural network to update parameters such as weight and biais. 

Our algorithm calculates dW and db according to dout, which the first neural network output.
We then need to send the "re-estimated" value in other hidden layer by calculating dX.

\end{enumerate}

\underline{ReLu activation}
\begin{itemize}
\item Forward pass :

Still in order to achieve a re-definition of hyper-parameters, we can apply to our input Data for the linear forward pass, an activation function. In this algorithm, we use $ReLU(X) = max(0,X)$.

ReLU functions are particurlarly efficient for Non-linear complex functional mappings between the inputs and response variable.

If we do not apply a Activation function then the output signal would simply be a simple linear function.A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data.A Neural Network without Activation function would simply be a Linear regression Model.

\item Backward pass :
\end{itemize}
