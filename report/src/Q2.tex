Consider a neural network with a large number of hidden units relative to the input layer's size.
Such a network is a powerful machine learning system, yet, without regularization, it is prone to overfitting.
Neurons fron a layer can develop dependencies to neurons from the previous layers.
In order to prevent such a thing, we randomly shutdown or ``drop'' neurons by setting their outputs to 0.
This penalizes neurons co-dependencies and allows the net to yields higher results.
Let's explore how we determine whether a neuron should be ``dropped'', and how it affects the forward and backward passes.


\begin{figure}[!ht]
    \centering
    {{\includegraphics[scale = 0.50]{src/diagrams/dropout.png}}}  
\end{figure}

\begin{itemize}[topsep=-10pt]
\item \textbf{Forward pass :} 

During the forward pass we are modifying the network itself,
neurons are randomly dropped from the network such that each neuron can be dropped with probability $p$,
or kept with probability $q = 1- p$.
To do so, for each neuron, we simulate a draw from a Bernoulli distribution by drawing a sample $x$ from a uniform distribution
and dropping a neuron if $x < p$.

We only use dropout when training a net and as such, during training, a neuron's expected input will be scaled down by $q$ such that $o_i = q\sum_{i=1}^K x_i$, where $x_i$ is the neuron's $i^{th}$ input.
Thus, during testing, a neuron will be overloaded as the sum of its inputs will be unexpectedly large.
A solution to this problem could be to scale activation outputs by the $q$ at test time..
Yet, in the spirit of simplification,
we can instead scale activation outputs by the inverse keep rate $\frac{1}{q}$ during the training phase.


\item \textbf{Backward pass :} 

But how does a neural net know what weights it should apply to each input?
How are they initialized?
How do they evolve?
   

During the backpropagation we just need to consider the Dropout, neuron that have been dropped doesn't contribute to further updates, they consquently need to be dropped in the derivative matrix - coresponding weight won't be updated. Therefor mask need to be stored between forward an backward pass to know which neuron have been dropped.

\end{itemize}



