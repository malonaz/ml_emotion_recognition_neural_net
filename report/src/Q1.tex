How does a neural net generate an output or in our case, how does it decide which emotion to recognize?
The output is produced in a process known as the emph{forward pass}.
Instead of processing one data point at a time,
we have implemented our neural net to process data points in batches.
As such, the forward pass takes in N examples in a matrix \emph{X}, where each row contains a data point of dimension D.
Let us take a closer look at the neural net's inner workings, assuming it has already been trained.
Note that although all the math involved in this coursework is in vector form, I find it more intuitive to illustrate my explanations
using scalar equations.
Please assume the following mathematical notations for the rest of the report, unless stated otherwise.\\
\- - $w_{ij}$ is the weight from neuron $i$ to neuron $j$.\\
\- - $b_j$ is the bias of a neuron $j$.\\
\- - $x_i$ is the $i^{th}$ input of a neuron.
\begin{enumerate}
   \item Affine transformation
  
     \underline{Forward pass}: $o_j= \sum\limits_{i=0}^{I} (w_{ij} \times x_{i}) + b_j$, \\
     When a layer goes through the affine transformation step,
     every neuron in the layer sums its $I$ inputs weighted by their respective weight, then adds its bias.\\

     \underline{Backward pass}: $\frac{do_j}{dx_i} = w_{ij}$,\space\space $\frac{do_j}{dw_{ij}} = x_i$, \space\space$\frac{do_j}{db_j} = 1$\\ 
     

   \item ReLU Activation Function
     
     So what does a neuron do? It computes a weighted sum of its input and must now decide whether it should be ``fired''.
     Earlier we spoke of an activation function.
     It is the activation that decides decides where the neuron should ``fire'' or not.
     Without an activation function, a layer's output would simply be a simple linear transformation.
     Linear transformation are limited in their complexity and have a lesser ability to learn complex functional mappings from data.
     A neural network without an activation function would simply be a multivariate linear regression model.
     There exists many activation functions such as the sigmoid function,
     but in this coursework, we use the function known the ReLU activation function.\\
     \underline{Forward pass}: $ReLU(\textbf{X}) = max(0,\textbf{X})$, where $\textbf{X}$ is the output of each neuron in a layer.\\
     The ReLU function outputs the neuron's sum if it is greater than 0 or simply outputs 0.
     
     *Note that only hidden layers go through ReLU.
     Output layer's neurons always ``fire'' and output their sum or ``score'' assigned by the net for each class we are classifying on.
     
\end{enumerate}
