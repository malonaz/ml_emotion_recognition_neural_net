How does a neural net generate an output or in our case, how does it decide which emotion to recognize?
The output is produced in a process known as the emph{forward pass}.
Instead of processing one data point at a time,
we have implemented our neural net to process data points in batches.
As such, the forward pass takes in N examples in a matrix \emph{X}, where each row contains a data point of dimension D.
Let us take a closer look at the neural net's inner workings, assuming it has already been trained.
Note that although all the math involved in this coursework is in vector form, I find it more intuitive to illustrate my explanations
using scalar equations.
Please assume the following mathematical notations for the rest of the report, unless stated otherwise.\\
\- - $w_{ij}$ is the weight from neuron $i$ to neuron $j$.\\
\- - $b_j$ is the bias of a neuron $j$.\\
\- - $x_i$ is the $i^{th}$ input of a neuron.
\begin{itemize}
   \item \textbf{Affine transformation}
  
     \underline{Forward pass}: $o_j= \sum\limits_{i=1}^{I} (w_{ij} \times x_{i}) + b_j$, \\
     When a layer goes through the affine transformation step,
     every neuron in the layer sums its $I$ inputs weighted by their respective weight, then adds its bias.\\

     \underline{Backward pass}: $\frac{\partial o_j}{\partial x_i} = w_{ij}$,
     \space\space $\frac{\partial o_j}{\partial w_{ij}} = x_i$,
     \space\space$\frac{\partial o_j}{\partial b_j} = 1$\\ 
     For a neuron $j$ in a layer that has just gone through forward affine transformation,
     we want to compute the derivative of its output $o_j$ with respect to each of its weights, and bias.
     Take a look at the formulas derived above and notice that I have include $\frac{\partial o_j}{\partial x_i}$.
     While we are not directly interested in how $o_j$ changes with respect to its inputs, we compute it for other layers that are.
     Remember that the $I$ inputs of a neuron $j$ are actually the outputs of the previous layers' neurons.
     Using the chain rule, the previous layer will use $\frac{\partial o_j}{\partial x_i}$
     to compute its backward affine transformation gradients.

   \item \textbf{ReLU activation function}
   
     So what does a neuron do? It computes a weighted sum of its input and must now decide whether it should be ``fired''.
     Earlier we spoke of an activation function.
     It is the activation that decides decides where the neuron should ``fire'' or not.
     Without an activation function, a layer's output would simply be a simple linear transformation.
     Linear transformation are limited in their complexity and have a lesser ability to learn complex functional mappings from data.
     A neural network without an activation function would simply be a multivariate linear regression model.
     There exists many activation functions such as the sigmoid function,
     but in this coursework, we use the function known the ReLU activation function.\\
     
     \underline{Forward pass}: $ReLU(o_j) = max(0,o_j)$\\
     The ReLU function outputs the neuron's sum if it is greater than 0 or simply outputs 0.
     
     *Note that only hidden layers go through ReLU.
     Output layer neurons always ``fire'' and output their sum or ``score''.

     \underline{Backward pass}: $\frac{\partial ReLU(o_j)}{\partial o_j} = 1$ if $o_j < 0$, else 0.\\
     This partial derivative is self-explanatory.
     
\end{itemize}
