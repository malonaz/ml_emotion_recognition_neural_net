How does a neural net generate an output or in our case, how does it decide which emotion to recognize?
The output is produced in a process known as the emph{forward pass}.
Instead of processing one data point at a time,
we have implemented our neural net to process data points in batches.
As such, the forward pass takes in N examples in a matrix \emph{X}, where each row contains a data point of dimension D.
Let us take a closer look at the neural net's inner workings, assuming it has already been trained.

\begin{enumerate}
   \item Affine transformation
  
     \underline{Forward pass}: $\textbf{y = wX + b}$\\
     The affine transformation is simply a linear mapping.
     Each neuron sums its inputs by their respective weights and then adds its bias to the sum.\\
     Thus, each neuron produces a $sum = \sum_{k=0}^{N} w_i * x_i + b$,
     where $x_i$ is its i$^{th}$ input, $w_i$ its i$^{th}$ weight and $b$ its bias.
     
   \item ReLU Activation Function
     
     So what does a neuron do? It computes a weighted sum of its input and must now decide whether it should be ``fired''.
     Earlier we spoke of an activation function.
     It is the activation that decides decides where the neuron should ``fire'' or not.
     Without an activation function, a layer's output would simply be a simple linear transformation.
     Linear transformation are limited in their complexity and have a lesser ability to learn complex functional mappings from data.
     A neural network without an activation function would simply be a multivariate linear regression model.
     There exists many activation functions such as the sigmoid function,
     but in this coursework, we use the function known the ReLU activation function.\\
     \underline{Forward pass}: $ReLU(\textbf{X}) = max(0,\textbf{X})$, where $\textbf{X}$ is the output of each neuron in a layer.\\
     The ReLU function outputs the neuron's sum if it is greater than 0 or simply outputs 0.
     
     *Note that only hidden layers go through ReLU.
     Output layer's neurons always ``fire'' and output their sum or ``score'' assigned by the net for each class we are classifying on.
     
\end{enumerate}
